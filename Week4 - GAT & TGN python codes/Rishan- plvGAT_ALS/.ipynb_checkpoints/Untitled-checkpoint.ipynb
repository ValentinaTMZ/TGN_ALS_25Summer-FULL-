{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0af6f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jan 27 18:26:26 2024\n",
    "Rishan Patel, UCL, Bioelectronics Group.\n",
    "\n",
    "Building a dataset and testing a GNN model. \n",
    "\n",
    "FBCSP works very well. Clearly frequency information is important.\n",
    "Creating graphs based on full spectrum, raw data PLV. 3 Classes, L,R,Re.\n",
    "ICA not performed. \n",
    "Using 10 frequency bands, band power measures as features 22x10. \n",
    "Using a standard Graph Neural Network.\n",
    "\n",
    "To Do: \n",
    "- ICA for Brain Wave Isolation\n",
    "- Optimising parameters\n",
    "- Removing randomisation, and barching so that models update over sequential time\n",
    "- Check accuracies for each class, not aggregate\n",
    "\n",
    "Notes: \n",
    "    \n",
    "    \n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html#data-handling-of-graphs\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html\n",
    "\"\"\"\n",
    "import os\n",
    "from os.path import dirname, join as pjoin\n",
    "import scipy as sp\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import specgram\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.signal as sig\n",
    "import networkx as nx\n",
    "#import torch as torch\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import entropy\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.integrate import simps\n",
    "from sklearn.model_selection import KFold\n",
    "#from torch_geometric.loader import DataLoader\n",
    "#from torch.nn import Linear\n",
    "#import torch.nn.functional as F\n",
    "#from torch_geometric.nn import GCNConv, GATConv, GATv2Conv, GAT\n",
    "#from torch_geometric.nn import global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb87fbf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/taomingzhe/Desktop/UCL-2025 Summer Project/Week4/Rishan- plvGAT_ALS/OGFS1.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/taomingzhe/Desktop/UCL-2025 Summer Project/Week4/Rishan- plvGAT_ALS/OGFS1.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subject_number \u001b[38;5;129;01min\u001b[39;00m subject_numbers:\n\u001b[1;32m     12\u001b[0m     mat_fname \u001b[38;5;241m=\u001b[39m pjoin(data_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOGFS\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     mat_contents \u001b[38;5;241m=\u001b[39m \u001b[43msio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     subject_data[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m mat_contents[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubject\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m subject_number,subject_numbers,mat_fname,mat_contents,data_dir\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:224\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    225\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    226\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/taomingzhe/Desktop/UCL-2025 Summer Project/Week4/Rishan- plvGAT_ALS/OGFS1.mat'"
     ]
    }
   ],
   "source": [
    "# % Preparing Data\n",
    "data_dir = os.getcwd()\n",
    "\n",
    "# Define the subject numbers\n",
    "subject_numbers = [1, 2, 5, 9, 21, 31, 34, 39]\n",
    "\n",
    "# Dictionary to hold the loaded data for each subject\n",
    "subject_data = {}\n",
    "\n",
    "# Loop through the subject numbers and load the corresponding data\n",
    "for subject_number in subject_numbers:\n",
    "    mat_fname = pjoin(data_dir, f'OGFS{subject_number}.mat')\n",
    "    mat_contents = sio.loadmat(mat_fname)\n",
    "    subject_data[f'S{subject_number}'] = mat_contents[f'Subject{subject_number}']\n",
    "del subject_number,subject_numbers,mat_fname,mat_contents,data_dir\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501f558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Phase-GAT\n",
    "S1 = subject_data['S1'][:,:]\n",
    "\n",
    "\n",
    "def plvfcn(eegData):\n",
    "    numElectrodes = eegData.shape[1]\n",
    "    numTimeSteps = eegData.shape[0]\n",
    "    plvMatrix = np.zeros((numElectrodes, numElectrodes))\n",
    "    for electrode1 in range(numElectrodes):\n",
    "        for electrode2 in range(electrode1 + 1, numElectrodes):\n",
    "            phase1 = np.angle(sig.hilbert(eegData[:, electrode1]))\n",
    "            phase2 = np.angle(sig.hilbert(eegData[:, electrode2]))\n",
    "            phase_difference = phase2 - phase1\n",
    "            plv = np.abs(np.sum(np.exp(1j * phase_difference)) / numTimeSteps)\n",
    "            plvMatrix[electrode1, electrode2] = plv\n",
    "            plvMatrix[electrode2, electrode1] = plv\n",
    "    return plvMatrix\n",
    "\n",
    "def compute_plv(subject_data):\n",
    "    idx = ['L', 'R']\n",
    "    numElectrodes = subject_data['L'][0,1].shape[1]\n",
    "    plv = {field: np.zeros((numElectrodes, numElectrodes, subject_data.shape[1])) for field in idx}\n",
    "    for i, field in enumerate(idx):\n",
    "        for j in range(subject_data.shape[1]):\n",
    "            x = subject_data[field][0, j]\n",
    "            plv[field][:, :, j] = plvfcn(x)\n",
    "    l, r = plv['L'], plv['R']\n",
    "    yl, yr = np.zeros((subject_data.shape[1], 1)), np.ones((subject_data.shape[1], 1))\n",
    "    img = np.concatenate((l, r), axis=2)\n",
    "    y = np.concatenate((yl, yr), axis=0)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    return img, y\n",
    "\n",
    "plv, y = compute_plv(S1)\n",
    "\n",
    "def create_graphs(plv, threshold):\n",
    "    \n",
    "    graphs = []\n",
    "    for i in range(plv.shape[2]):\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(plv.shape[0]))\n",
    "        for u in range(plv.shape[0]):\n",
    "            for v in range(plv.shape[0]):\n",
    "                if u != v and plv[u, v, i] > threshold:\n",
    "                    G.add_edge(u, v, weight=plv[u, v, i])\n",
    "        graphs.append(G)\n",
    "    return graphs\n",
    "threshold = 0.5\n",
    "graphs = create_graphs(plv, threshold)\n",
    "\n",
    "numElectrodes = S1['L'][0,1].shape[1]\n",
    "adj = np.zeros([numElectrodes, numElectrodes, len(graphs)])\n",
    "for i, G in enumerate(graphs):\n",
    "    adj[:, :, i] = nx.to_numpy_array(G)\n",
    "\n",
    "#% Initialize an empty list to store edge indices\n",
    "edge_indices = [] # % Edge indices are a list of source and target nodes in a graph. Think of it like the adjacency matrix\n",
    "\n",
    "# Iterate over the adjacency matrices\n",
    "for i in range(adj.shape[2]):\n",
    "    # Initialize lists to store source and target nodes\n",
    "    source_nodes = []\n",
    "    target_nodes = []\n",
    "    \n",
    "    # Iterate through each element of the adjacency matrix\n",
    "    for row in range(adj.shape[0]):\n",
    "        for col in range(adj.shape[1]):\n",
    "            # Check if there's an edge\n",
    "            if adj[row, col, i] >= threshold:\n",
    "                # Add source and target nodes to the lists\n",
    "                source_nodes.append(row)\n",
    "                target_nodes.append(col)\n",
    "            else:\n",
    "                # If no edge exists, add placeholder zeros to maintain size\n",
    "                source_nodes.append(0)\n",
    "                target_nodes.append(0)\n",
    "    \n",
    "    # Create edge index as a LongTensor\n",
    "    edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "    \n",
    "    # Append edge index to the list\n",
    "    edge_indices.append(edge_index)\n",
    "\n",
    "# Stack all edge indices along a new axis to create a 2D tensor\n",
    "edge_indices = torch.stack(edge_indices, dim=-1)\n",
    "\n",
    "del col,edge_index,i,row,source_nodes,target_nodes\n",
    "#%\n",
    "\n",
    "def aggregate_eeg_data(S1,band): #%% This is to get the feat vector\n",
    "    \"\"\"\n",
    "    Aggregate EEG data for each class.\n",
    "\n",
    "    Parameters:\n",
    "        S1 (dict): Dictionary containing EEG data for each class. Keys are class labels, \n",
    "                   values are arrays of shape (2, num_samples, num_channels), where the first dimension\n",
    "                   corresponds to EEG data (index 0) and frequency data (index 1).\n",
    "\n",
    "    Returns:\n",
    "        l (ndarray): Aggregated EEG data for class 'L'.\n",
    "        r (ndarray): Aggregated EEG data for class 'R'.\n",
    "    \"\"\"\n",
    "    idx = ['L', 'R']\n",
    "    numElectrodes = S1['L'][0,1].shape[1];\n",
    "    max_sizes = {field: 0 for field in idx}\n",
    "\n",
    "    # Find the maximum size of EEG data for each class\n",
    "    for field in idx:\n",
    "        for i in range(S1[field].shape[1]):\n",
    "            max_sizes[field] = max(max_sizes[field], S1[field][0, i].shape[0])\n",
    "\n",
    "    # Initialize arrays to store aggregated EEG data\n",
    "    l = np.zeros((max_sizes['L'], numElectrodes, S1['L'].shape[1]))\n",
    "    r = np.zeros((max_sizes['R'], numElectrodes, S1['R'].shape[1]))\n",
    "\n",
    "    # Loop through each sample\n",
    "    for i in range(S1['L'].shape[1]):\n",
    "        for j, field in enumerate(idx):\n",
    "            x = S1[field][0, i]  # EEG data for the current sample\n",
    "            # Resize x to match the maximum size\n",
    "            resized_x = np.zeros((max_sizes[field], 22))\n",
    "            resized_x[:x.shape[0], :] = x\n",
    "            # Add the resized EEG data to the respective array\n",
    "            if field == 'L':\n",
    "                l[:, :, i] += resized_x\n",
    "            elif field == 'R':\n",
    "                r[:, :, i] += resized_x\n",
    "\n",
    "    l = l[..., np.newaxis]\n",
    "    l = np.copy(l) * np.ones(len(band)-1)\n",
    "\n",
    "    r = r[..., np.newaxis]\n",
    "    r = np.copy(r) * np.ones(len(band)-1)\n",
    "    \n",
    "    return l, r\n",
    "\n",
    "band = list(range(8, 41, 4))\n",
    "l,r = aggregate_eeg_data(S1,band)\n",
    "l,r = np.transpose(l,[1,0,2,3]),np.transpose(r,[1,0,2,3])\n",
    "\n",
    "\n",
    "fs = 256\n",
    "\n",
    "def bandpass(data: np.ndarray, edges: list[float], sample_rate: float, poles: int = 5):\n",
    "    sos = sig.butter(poles, edges, 'bandpass', fs=sample_rate, output='sos')\n",
    "    filtered_data = sig.sosfiltfilt(sos, data)\n",
    "    return filtered_data\n",
    "\n",
    "for i in range(l.shape[3]):\n",
    "    bp = [band[i],band[i+1]]\n",
    "    for j in range(l.shape[2]):\n",
    "        l[:,:,j,i] = bandpass(l[:,:,j,i],bp,sample_rate=fs)\n",
    "        r[:,:,j,i] = bandpass(r[:,:,j,i],bp,sample_rate=fs)\n",
    "\n",
    "#% Convert data from 22x1288x150xF to 22xFx150 where nodes x features x sample\n",
    "# features are BP of the band. \n",
    "\n",
    "def bandpower(data,low,high):\n",
    "\n",
    "    fs = 256\n",
    "    # Define window length (2s)\n",
    "    win = 2* fs\n",
    "    freqs, psd = signal.welch(data, fs, nperseg=win)\n",
    "    \n",
    "    # Find intersecting values in frequency vector\n",
    "    idx_delta = np.logical_and(freqs >= low, freqs <= high)\n",
    "    \n",
    "    # # Plot the power spectral density and fill the delta area\n",
    "    # plt.figure(figsize=(7, 4))\n",
    "    # plt.plot(freqs, psd, lw=2, color='k')\n",
    "    # plt.fill_between(freqs, psd, where=idx_delta, color='skyblue')\n",
    "    # plt.xlabel('Frequency (Hz)')\n",
    "    # plt.ylabel('Power spectral density (uV^2 / Hz)')\n",
    "    # plt.xlim([0, 40])\n",
    "    # plt.ylim([0, psd.max() * 1.1])\n",
    "    # plt.title(\"Welch's periodogram\")\n",
    "    \n",
    "    # Frequency resolution\n",
    "    freq_res = freqs[1] - freqs[0]  # = 1 / 4 = 0.25\n",
    "    \n",
    "    # Compute the absolute power by approximating the area under the curve\n",
    "    power = simps(psd[idx_delta], dx=freq_res)\n",
    "    \n",
    "    return power\n",
    "\n",
    "def bandpowercalc(l,band,fs):   \n",
    "    x = np.zeros([l.shape[0],l.shape[3],l.shape[2]])\n",
    "    for i in range(l.shape[0]): #node\n",
    "        for j in range(l.shape[2]): #sample\n",
    "            for k in range(0,l.shape[3]): #band\n",
    "                data = l[i,:,j,k]\n",
    "                low = band[k]\n",
    "                high = band[k+1]\n",
    "                x[i,k,j] = bandpower(data,low,high)\n",
    "\n",
    "    return x\n",
    "                \n",
    "l = bandpowercalc(l,band,fs)\n",
    "r = bandpowercalc(r,band,fs)\n",
    "\n",
    "x = np.concatenate([l,r],axis=2)\n",
    "x = torch.tensor(x,dtype=torch.float32)\n",
    "\n",
    "del r,l,S1,i,j,G,bp \n",
    "#%\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "data_list = []\n",
    "for i in range(np.size(adj,2)):\n",
    "    data_list.append(Data(x=x[:, :, i], edge_index=edge_indices[:,:,i], y=y[i, 0]))\n",
    "\n",
    "\n",
    "# def split(data_list, train_p, val_p, test_p):\n",
    "#     num_samples = len(data_list)\n",
    "#     a = num_samples // 3\n",
    "#     class_splits = [\n",
    "#         int(train_p * a), int(train_p * a), int(train_p * a),\n",
    "#         int(val_p * a), int(val_p * a), int(val_p * a),\n",
    "#         int(test_p * a), int(test_p * a), int(test_p * a)\n",
    "#     ]\n",
    "#     indices = list(range(num_samples))\n",
    "#     #np.random.shuffle(indices)\n",
    "    \n",
    "#     train_indices = indices[:sum(class_splits[:3])]\n",
    "#     val_indices = indices[sum(class_splits[:3]):sum(class_splits[:6])]\n",
    "#     test_indices = indices[sum(class_splits[:6]):]\n",
    "\n",
    "#     train_data = [data_list[i] for i in train_indices]\n",
    "#     val_data = [data_list[i] for i in val_indices]\n",
    "#     test_data = [data_list[i] for i in test_indices]\n",
    "\n",
    "#     return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "# train,val,test = split(data_list,0.5,0,0.5)\n",
    "\n",
    "#% split data so its sequentially 1,2,3\n",
    "datal =[]\n",
    "datar =[]\n",
    "size = len(data_list)\n",
    "idx = size//2\n",
    "c = [0,idx,idx*2,idx*3]\n",
    "\n",
    "datal = data_list[c[0]:c[1]]\n",
    "datar = data_list[c[1]:c[2]]\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for i in range(idx):\n",
    "    x = [datal[i],datar[i]] #datare[i]]\n",
    "    data_list.extend(x)\n",
    "\n",
    "\n",
    "size = len(data_list)\n",
    "\n",
    "# Initialize KFold with 5 splits\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store highest test accuracies of each fold\n",
    "highest_test_accuracies = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(data_list)):\n",
    "    # Create training and testing sets\n",
    "    train = [data_list[i] for i in train_idx]\n",
    "    test = [data_list[i] for i in test_idx]\n",
    "    \n",
    "    # Print the number of samples in train and test sets for each fold\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    print(f\"Train size: {len(train)}\")\n",
    "    print(f\"Test size: {len(test)}\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    torch.manual_seed(12345)\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=8, shuffle=False)\n",
    "    test_loader = DataLoader(test, batch_size=8, shuffle=False)\n",
    "\n",
    "    for step, data in enumerate(train_loader):\n",
    "        print(f'Step {step + 1}:')\n",
    "        print('=======')\n",
    "        print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "        print(data)\n",
    "        print()\n",
    "    \n",
    "    # class GCN(torch.nn.Module):\n",
    "    #     def __init__(self, hidden_channels):\n",
    "    #         super(GCN, self).__init__()\n",
    "    #         torch.manual_seed(12345)\n",
    "    #         self.conv1 = GCNConv(8, hidden_channels)  # num node features\n",
    "    #         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "    #         self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "    #         self.lin = Linear(hidden_channels, 2)  # num of classes\n",
    "    class GAT(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels,heads):\n",
    "            super(GAT, self).__init__()\n",
    "            torch.manual_seed(12345)\n",
    "            self.conv1 = GATv2Conv(8, hidden_channels, heads=heads, concat=True)  # num node features\n",
    "            self.conv2 = GATv2Conv(hidden_channels*heads, hidden_channels, heads=heads, concat=True)\n",
    "            self.conv3 = GATv2Conv(hidden_channels*heads, hidden_channels, heads=heads,concat=True)\n",
    "            self.lin = Linear(hidden_channels*heads, 2)  # num of classes\n",
    "\n",
    "        def forward(self, x, edge_index, batch):\n",
    "            # 1. Obtain node embeddings \n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = self.conv3(x, edge_index)\n",
    "\n",
    "            # 2. Readout layer\n",
    "            x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "            # 3. Apply a final classifier\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.lin(x)\n",
    "            \n",
    "            return x\n",
    "\n",
    "    model = GAT(hidden_channels=64,heads=2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "            loss = criterion(out, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            out = model(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "    optimal = [0, 0, 0]\n",
    "    for epoch in range(1, 200):\n",
    "        train()\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        av_acc = np.mean([train_acc, test_acc])\n",
    "        \n",
    "        if test_acc > optimal[2]:\n",
    "            optimal[0] = av_acc\n",
    "            optimal[1] = train_acc\n",
    "            optimal[2] = test_acc\n",
    "        \n",
    "        print(f'Opt_Av: {optimal[0]:.4f}, Opt_Train: {optimal[1]:.4f}, Opt_Test: {optimal[2]:.4f}')\n",
    "\n",
    "    # Store the highest test accuracy of the current fold\n",
    "    highest_test_accuracies.append(optimal[2])\n",
    "\n",
    "# Calculate and print the mean of the highest test accuracies across all folds\n",
    "mean_highest_accuracy = np.mean(highest_test_accuracies)\n",
    "print(f'Mean Highest Test Accuracy Across 5 Folds: {mean_highest_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
